{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### boolq for moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.44.0.dev0 and datasets v2.20.0\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, default_data_collator, AdamW, \n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "from transformerlab.MoE import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__}\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "boolq = load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)\n",
    "\n",
    "def tokenize_and_encode(examples): \n",
    "    return tokenizer(examples['question'], examples['passage'], truncation=\"only_second\")\n",
    "\n",
    "boolq_enc = boolq.map(tokenize_and_encode, batched=True)\n",
    "\n",
    "train_ds = boolq_enc[\"train\"]\n",
    "eval_ds = boolq_enc[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### training_args & trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "class PruningTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, initial_threshold=1., final_threshold=0.1, initial_warmup=1, final_warmup=2, final_lambda=0.,\n",
    "                 mask_scores_learning_rate=0., **kwargs): \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.initial_threshold = initial_threshold\n",
    "        self.final_threshold = final_threshold\n",
    "        self.initial_warmup = initial_warmup\n",
    "        self.final_warmup = final_warmup\n",
    "        self.final_lambda = final_lambda\n",
    "        self.mask_scores_learning_rate = mask_scores_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decide the training parameters\n",
    "\n",
    "class PruningTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if self.args.max_steps > 0:\n",
    "            self.t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            self.t_total = len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "            \n",
    "        \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        no_decay = [\"bias\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"mask_scores_1\" in n and p.requires_grad],\n",
    "                \"lr\": self.args.mask_scores_learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"mask_scores_2\" in n and p.requires_grad],\n",
    "                \"lr\": self.args.mask_scores_learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"gate_lin\" in n and p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"gate_lin\" in n and p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        # 打印 optimizer_grouped_parameters 的内容\n",
    "        for group in optimizer_grouped_parameters:\n",
    "            print(f\"Group with lr {group['lr']}:\")\n",
    "            for param in group['params']:\n",
    "                print(f\"  - {param.shape}\")\n",
    "                \n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=self.t_total\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \n",
    "        threshold, regu_lambda = self._schedule_threshold(\n",
    "            step=self.state.global_step+1,\n",
    "            total_step=self.t_total,\n",
    "            warmup_steps=self.args.warmup_steps,\n",
    "            final_threshold=self.args.final_threshold,\n",
    "            initial_threshold=self.args.initial_threshold,\n",
    "            final_warmup=self.args.final_warmup,\n",
    "            initial_warmup=self.args.initial_warmup,\n",
    "            final_lambda=self.args.final_lambda,\n",
    "        )\n",
    "        inputs[\"threshold\"] = threshold  \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        ### from nn_pruning\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    \n",
    "    def _schedule_threshold(\n",
    "        self,\n",
    "        step: int,\n",
    "        total_step: int,\n",
    "        warmup_steps: int,\n",
    "        initial_threshold: float,\n",
    "        final_threshold: float,\n",
    "        initial_warmup: int,\n",
    "        final_warmup: int,\n",
    "        final_lambda: float,\n",
    "    ):\n",
    "        if step <= initial_warmup * warmup_steps:\n",
    "            threshold = initial_threshold\n",
    "        elif step > (total_step - final_warmup * warmup_steps):\n",
    "            threshold = final_threshold\n",
    "        else:\n",
    "            spars_warmup_steps = initial_warmup * warmup_steps\n",
    "            spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n",
    "            mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n",
    "            threshold = final_threshold + (initial_threshold - final_threshold) * (mul_coeff ** 3)\n",
    "        regu_lambda = final_lambda * threshold / final_threshold\n",
    "        return threshold, regu_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config & model\n",
    "masked_config = MaskedBertConfig(pruning_method='topK', mask_init='constant', mask_scale=0.0)\n",
    "\n",
    "bert_model = MaskedBertForSequenceClassification.from_pretrained(bert_ckpt, config=masked_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 904.07 MB\n"
     ]
    }
   ],
   "source": [
    "### measure the model_size\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "model_size = get_model_size(bert_model)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.0\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "\n",
    "args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    weight_decay=0.01,\n",
    "    initial_threshold=initial_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,\n",
    "    fp16=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3377951/2845288297.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_score = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group with lr 0.0:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "Group with lr 0.0:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pruning_trainer.create_optimizer_and_scheduler(num_training_steps=pruning_trainer.t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_prune(final_threshold, num_train_epochs, mask_scores_learning_rate=1e-2):\n",
    "    pruning_trainer.args.final_threshold = final_threshold\n",
    "    pruning_trainer.args.mask_scores_learning_rate = mask_scores_learning_rate\n",
    "    pruning_trainer.args.num_train_epochs = num_train_epochs\n",
    "    pruning_trainer.args.warmup_steps = pruning_trainer.args.logging_steps * num_train_epochs * 0.1\n",
    "    print(f\"Fine-pruning {(1-pruning_trainer.args.final_threshold)*100:.2f}% of weights with lr = {pruning_trainer.args.learning_rate} and mask_lr = {pruning_trainer.args.mask_scores_learning_rate} and {pruning_trainer.args.warmup_steps} warmup steps\")\n",
    "    pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-pruning 30.00% of weights with lr = 2e-05 and mask_lr = 0.01 and 353.40000000000003 warmup steps\n",
      "Group with lr 0.01:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "Group with lr 0.01:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([2, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae64f8749ee4151879501f3a9e49cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3377951/3905390559.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_prune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3377951/733587535.py\u001b[0m in \u001b[0;36mfine_prune\u001b[0;34m(final_threshold, num_train_epochs, mask_scores_learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpruning_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruning_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_train_epochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fine-pruning {(1-pruning_trainer.args.final_threshold)*100:.2f}% of weights with lr = {pruning_trainer.args.learning_rate} and mask_lr = {pruning_trainer.args.mask_scores_learning_rate} and {pruning_trainer.args.warmup_steps} warmup steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpruning_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3377951/1674225262.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m### from nn_pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fine_prune(0.7, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
