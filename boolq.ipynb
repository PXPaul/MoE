{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### boolq for moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.44.0.dev0 and datasets v2.20.0\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "datasets.logging.set_verbosity_error()\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, default_data_collator, AdamW, \n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "from distilbert import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using transformers v{transformers.__version__} and datasets v{datasets.__version__}\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "boolq = load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_ckpt = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)\n",
    "\n",
    "def tokenize_and_encode(examples): \n",
    "    return tokenizer(examples['question'], examples['passage'], truncation=\"only_second\") # max_length=128)\n",
    "\n",
    "boolq_enc = boolq.map(tokenize_and_encode, batched=True)\n",
    "\n",
    "train_ds = boolq_enc[\"train\"]\n",
    "eval_ds = boolq_enc[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### training_args & trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "class PruningTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, initial_threshold=1., final_threshold=0.1, initial_warmup=1, final_warmup=2, final_lambda=0.,\n",
    "                 mask_scores_learning_rate=0., **kwargs): \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.initial_threshold = initial_threshold\n",
    "        self.final_threshold = final_threshold\n",
    "        self.initial_warmup = initial_warmup\n",
    "        self.final_warmup = final_warmup\n",
    "        self.final_lambda = final_lambda\n",
    "        self.mask_scores_learning_rate = mask_scores_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decide the training parameters\n",
    "\n",
    "class PruningTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if self.args.max_steps > 0:\n",
    "            self.t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            self.t_total = len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "            \n",
    "        \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"NoNorm.weight\", \"layer_norm.weight\", \"layernorm_embedding.weight\",\n",
    "                    \"final_layer_norm.weight\"]\n",
    "        #[\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if \"mask_score\" in n and p.requires_grad],\n",
    "                \"lr\": self.args.mask_scores_learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if \"mask_score\" not in n and p.requires_grad and not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": self.args.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if \"mask_score\" not in n and p.requires_grad and any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.args.learning_rate,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        for group in optimizer_grouped_parameters:\n",
    "            print(f\"Group with lr {group['lr']}:\")\n",
    "            for param in group['params']:\n",
    "                print(f\"  - {param.shape}\")\n",
    "                \n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=self.t_total\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \n",
    "        threshold, regu_lambda = self._schedule_threshold(\n",
    "            step=self.state.global_step+1,\n",
    "            total_step=self.t_total,\n",
    "            warmup_steps=self.args.warmup_steps,\n",
    "            final_threshold=self.args.final_threshold,\n",
    "            initial_threshold=self.args.initial_threshold,\n",
    "            final_warmup=self.args.final_warmup,\n",
    "            initial_warmup=self.args.initial_warmup,\n",
    "            final_lambda=self.args.final_lambda,\n",
    "        )\n",
    "        inputs[\"threshold\"] = threshold  \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        ### from nn_pruning\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    \n",
    "    def _schedule_threshold(\n",
    "        self,\n",
    "        step: int,\n",
    "        total_step: int,\n",
    "        warmup_steps: int,\n",
    "        initial_threshold: float,\n",
    "        final_threshold: float,\n",
    "        initial_warmup: int,\n",
    "        final_warmup: int,\n",
    "        final_lambda: float,\n",
    "    ):\n",
    "        if step <= initial_warmup * warmup_steps:\n",
    "            threshold = initial_threshold\n",
    "        elif step > (total_step - final_warmup * warmup_steps):\n",
    "            threshold = final_threshold\n",
    "        else:\n",
    "            spars_warmup_steps = initial_warmup * warmup_steps\n",
    "            spars_schedu_steps = (final_warmup + initial_warmup) * warmup_steps\n",
    "            mul_coeff = 1 - (step - spars_warmup_steps) / (total_step - spars_schedu_steps)\n",
    "            threshold = final_threshold + (initial_threshold - final_threshold) * (mul_coeff ** 3)\n",
    "        regu_lambda = final_lambda * threshold / final_threshold\n",
    "        return threshold, regu_lambda\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config & model\n",
    "masked_config = MaskedDistilBertConfig(pruning_method='topK', mask_init='constant', mask_scale=0.0)\n",
    "\n",
    "bert_model = MaskedDistilBertForSequenceClassification.from_pretrained(bert_ckpt, config=masked_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 536.87 MB\n"
     ]
    }
   ],
   "source": [
    "### measure the model_size\n",
    "\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "model_size = get_model_size(bert_model)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "logging_steps = len(train_ds) // batch_size\n",
    "\n",
    "# pruning params\n",
    "initial_threshold = 1.0\n",
    "initial_warmup = 1\n",
    "final_warmup = 2\n",
    "final_lambda = 0\n",
    "\n",
    "args = PruningTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=logging_steps,\n",
    "    weight_decay=0.01,\n",
    "    initial_threshold=initial_threshold,\n",
    "    initial_warmup=initial_warmup,\n",
    "    final_warmup=final_warmup,\n",
    "    final_lambda=final_lambda,\n",
    "    disable_tqdm=False,\n",
    "    report_to=None,\n",
    "    fp16=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118416/2845288297.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_score = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "accuracy_score = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_trainer = PruningTrainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_prune(final_threshold, num_train_epochs, mask_scores_learning_rate=1e-2):\n",
    "    pruning_trainer.args.final_threshold = final_threshold\n",
    "    pruning_trainer.args.mask_scores_learning_rate = mask_scores_learning_rate\n",
    "    pruning_trainer.args.num_train_epochs = num_train_epochs\n",
    "    pruning_trainer.args.warmup_steps = pruning_trainer.args.logging_steps * num_train_epochs * 0.1\n",
    "    print(f\"Fine-pruning {(1-pruning_trainer.args.final_threshold)*100:.2f}% of weights with lr = {pruning_trainer.args.learning_rate} and mask_lr = {pruning_trainer.args.mask_scores_learning_rate} and {pruning_trainer.args.warmup_steps} warmup steps\")\n",
    "    pruning_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-pruning 80.00% of weights with lr = 2e-05 and mask_lr = 0.01 and 353.40000000000003 warmup steps\n",
      "Group with lr 0.01:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 768])\n",
      "  - torch.Size([3072, 768])\n",
      "  - torch.Size([3, 768])\n",
      "  - torch.Size([768, 3072])\n",
      "  - torch.Size([2, 768])\n",
      "Group with lr 2e-05:\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([3072])\n",
      "  - torch.Size([3])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([768])\n",
      "  - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da3f7ce18a544018d0d4ef90e9a5110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb5f61935f940728136f1fb6c1d0c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6623115539550781, 'eval_accuracy': 0.6241590214067279, 'eval_runtime': 24.4464, 'eval_samples_per_second': 133.762, 'eval_steps_per_second': 4.213, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d382896c8c1b4509a375a40236c277db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6578486561775208, 'eval_accuracy': 0.6055045871559633, 'eval_runtime': 24.5425, 'eval_samples_per_second': 133.239, 'eval_steps_per_second': 4.197, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/lip/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f405025dd34df590414006c5083dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6363855600357056, 'eval_accuracy': 0.6437308868501529, 'eval_runtime': 24.5165, 'eval_samples_per_second': 133.38, 'eval_steps_per_second': 4.201, 'epoch': 3.0}\n",
      "{'train_runtime': 423.8203, 'train_samples_per_second': 66.729, 'train_steps_per_second': 2.088, 'train_loss': 0.6626059322033898, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "fine_prune(0.2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_model = pruning_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140737610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2118416/2845042334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoe_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, labels, threshold)\u001b[0m\n\u001b[1;32m    515\u001b[0m     ):\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         outputs = self.distilbert(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, threshold)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 143\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, threshold)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, threshold)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     ):\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, threshold)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     ):\n\u001b[0;32m--> 260\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, threshold)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     ):\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, threshold)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mmask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopKBinarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_scores_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mmask2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopKBinarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_scores_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mmask3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopKBinarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_scores_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/transformerlab/distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, inputs, threshold)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# Get the subnetwork by sorting the inputs and using the top threshold %\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import time\n",
    "\n",
    "question = \"Is it raining outside?\"\n",
    "passage = \"The weather forecast predicts heavy rain today.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(question, passage, truncation=\"only_second\", return_tensors=\"pt\")\n",
    "\n",
    "moe_model = pruning_trainer.model\n",
    "device = torch.device('cpu')\n",
    "moe_model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# æµ‹è¯•å»¶è¿Ÿ\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    outputs = moe_model(**inputs)\n",
    "    end_time = time.time()\n",
    "\n",
    "# è®¡ç®—å»¶è¿Ÿ\n",
    "latency = end_time - start_time\n",
    "print(f\"Latency: {latency:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
