{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######MoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class TopKBinarizer(Function):\n",
    "    \"\"\"\n",
    "    Top-k Binarizer.\n",
    "    Computes a binary mask M from a real value matrix S such that `M_{i,j} = 1` if and only if `S_{i,j}`\n",
    "    is among the k% highest values of S.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs: torch.tensor, threshold: float):\n",
    "        # Get the subnetwork by sorting the inputs and using the top threshold %\n",
    "        mask = inputs.clone()\n",
    "        _, idx = inputs.flatten().sort(descending=True)\n",
    "        j = int(threshold * inputs.numel())\n",
    "\n",
    "        # flat_out and mask access the same memory.\n",
    "        flat_out = mask.flatten()\n",
    "        flat_out[idx[j:]] = 0\n",
    "        flat_out[idx[:j]] = 1\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradOutput):\n",
    "        return gradOutput, None\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.out = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "        # Trainable mask scores for attention heads\n",
    "        self.mask_score_heads_1 = nn.Parameter(torch.ones(self.num_attention_heads))\n",
    "        self.mask_score_heads_2 = nn.Parameter(torch.ones(self.num_attention_heads))\n",
    "\n",
    "        self.topk_threshold = config.topk_threshold\n",
    "\n",
    "        # Gate for selecting mask: small two-layer fully connected network\n",
    "        self.gate_heads = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 2)  # 2 possible masks for heads\n",
    "        )\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Generate the corresponding binary mask for attention heads\n",
    "        mask_heads_1 = TopKBinarizer.apply(self.mask_score_heads_1, self.topk_threshold)\n",
    "        mask_heads_2 = TopKBinarizer.apply(self.mask_score_heads_2, self.topk_threshold)\n",
    "\n",
    "        # Compute the gate scores for attention heads\n",
    "        gate_scores_heads = self.gate_heads(hidden_states.mean(dim=1))\n",
    "        gate_probs_heads = F.softmax(gate_scores_heads, dim=-1)\n",
    "\n",
    "        # Select the mask with the highest probability for attention heads\n",
    "        selected_mask_index_heads = torch.argmax(gate_probs_heads, dim=1)\n",
    "\n",
    "        if selected_mask_index_heads[0] == 0:\n",
    "            selected_mask_heads = mask_heads_1\n",
    "        else:\n",
    "            selected_mask_heads = mask_heads_2\n",
    "\n",
    "        # Apply mask to the attention heads\n",
    "        # `selected_mask_heads` has shape [num_attention_heads]\n",
    "        # We need to expand it to [batch_size, num_attention_heads, seq_length, seq_length]\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "        masked_attention_heads = selected_mask_heads.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand(batch_size, self.num_attention_heads, seq_length, seq_length)\n",
    "\n",
    "        # Forward pass without modifying q, k, v\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Apply mask to the attention probs\n",
    "        attention_probs = attention_probs * masked_attention_heads\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, 4 * config.hidden_size) \n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * config.hidden_size, config.hidden_size) \n",
    "        \n",
    "        # Trainable mask scores\n",
    "        self.mask_score_1 = nn.Parameter(torch.ones(4 * config.hidden_size, config.hidden_size))\n",
    "        self.mask_score_2 = nn.Parameter(torch.ones(4 * config.hidden_size, config.hidden_size))\n",
    "        \n",
    "        self.topk_threshold = config.topk_threshold\n",
    "\n",
    "        # Gate for selecting mask: small two-layer fully connected network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate the corresponding binary mask\n",
    "        mask1 = TopKBinarizer.apply(self.mask_score_1, self.topk_threshold)\n",
    "        mask2 = TopKBinarizer.apply(self.mask_score_2, self.topk_threshold)\n",
    "        \n",
    "        # Compute the gate scores\n",
    "        gate_scores = self.gate(x.mean(dim=1))  # Using mean of x as a simple representation\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "        \n",
    "        # Select the mask with the highest probability\n",
    "        selected_mask_index = torch.argmax(gate_probs, dim=1)\n",
    "        \n",
    "        if selected_mask_index[0] == 0:\n",
    "            selected_mask = mask1\n",
    "        else:\n",
    "            selected_mask = mask2\n",
    "        \n",
    "        # Apply mask to the dense layer's weights\n",
    "        masked_weights = self.fc1.weight * selected_mask\n",
    "\n",
    "        # Forward pass with masked weights\n",
    "        x = F.linear(x, masked_weights, self.fc1.bias)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        attn_output = self.attn(self.ln1(x), attention_mask)\n",
    "        x = x + attn_output\n",
    "        mlp_output = self.mlp(self.ln2(x))\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class BERTConfig:\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 4 * hidden_size \n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    num_hidden_layers: int = 12\n",
    "    vocab_size: int = 30522\n",
    "    max_position_embeddings: int = 512\n",
    "    topk_threshold: float = 0.5\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([Block(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        token_embeddings = self.embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained BERT model weights from HuggingFace\"\"\"\n",
    "        print(f\"Loading weights from pretrained BERT: {model_type}\")\n",
    "\n",
    "        # Instantiate the BERT model\n",
    "        config_args = {\n",
    "            'bert-base-uncased': dict(num_hidden_layers=12, num_attention_heads=12, hidden_size=768, intermediate_size=4 * 768),\n",
    "            'bert-large-uncased': dict(num_hidden_layers=24, num_attention_heads=16, hidden_size=1024, intermediate_size=4 * 1024),\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 30522\n",
    "        config_args['max_position_embeddings'] = 512\n",
    "        config = BERTConfig(**config_args)\n",
    "        model = BERT(config)\n",
    "\n",
    "        # Load HuggingFace BERT model for weight extraction\n",
    "        model_hf = BertModel.from_pretrained(model_type)\n",
    "\n",
    "        # Copy weights from HuggingFace model to our model\n",
    "        pretrained_dict = model_hf.state_dict()\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # Filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "\n",
    "        # Overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "        # Load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "#####for test\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class TopKBinarizer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs: torch.Tensor, threshold: float):\n",
    "        mask = inputs.clone()\n",
    "        _, idx = inputs.flatten().sort(descending=True)\n",
    "        j = int(threshold * inputs.numel())\n",
    "        \n",
    "        flat_out = mask.flatten()\n",
    "        flat_out[idx[j:]] = 0\n",
    "        flat_out[idx[:j]] = 1\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradOutput):\n",
    "        return gradOutput, None\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 4 * hidden_size) \n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * hidden_size, hidden_size) \n",
    "        \n",
    "        # Trainable mask scores\n",
    "        self.mask_score_1 = nn.Parameter(torch.rand(4 * hidden_size, hidden_size))\n",
    "        self.mask_score_2 = nn.Parameter(torch.rand(4 * hidden_size, hidden_size))\n",
    "        \n",
    "        self.topk_threshold = 0.2\n",
    "\n",
    "        # Gate for selecting mask: small two-layer fully connected network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate the corresponding binary mask\n",
    "        mask1 = TopKBinarizer.apply(self.mask_score_1, self.topk_threshold)\n",
    "        mask2 = TopKBinarizer.apply(self.mask_score_2, self.topk_threshold)\n",
    "        \n",
    "        # Compute the gate scores\n",
    "        gate_scores = self.gate(x.mean(dim=1))  # Using mean of x as a simple representation\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "        \n",
    "        # Select the mask with the highest probability\n",
    "        selected_mask_index = torch.argmax(gate_probs, dim=1) \n",
    "        \n",
    "        # Apply the appropriate mask to the dense layer's weights for each sample\n",
    "        masked_weights = self.fc1.weight.unsqueeze(0).repeat(x.size(0), 1, 1)  # Repeat weights for batch size\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            if selected_mask_index[i] == 0:\n",
    "                masked_weights[i] *= mask1\n",
    "            else:\n",
    "                masked_weights[i] *= mask2\n",
    "\n",
    "        # Forward pass with masked weights\n",
    "        x = torch.bmm(x, masked_weights.transpose(1, 2))  # Batch matrix multiplication\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return selected_mask_index\n",
    "\n",
    "#example\n",
    "hidden_size = 3\n",
    "mlp = MLP(hidden_size)\n",
    "x = torch.randn(2, 3, hidden_size)  # 2个sample，3个时间步，每个时间步hidden_size维度\n",
    "\n",
    "# 计算selected_mask\n",
    "output = mlp(x)\n",
    "print(output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.autograd import Function\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class TopKBinarizer(Function):\n",
    "    \"\"\"\n",
    "    Top-k Binarizer.\n",
    "    Computes a binary mask M from a real value matrix S such that `M_{i,j} = 1` if and only if `S_{i,j}`\n",
    "    is among the k% highest values of S.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs: torch.Tensor, threshold: float):\n",
    "        # Get the subnetwork by sorting the inputs and using the top threshold %\n",
    "        mask = inputs.clone()\n",
    "        _, idx = inputs.flatten().sort(descending=True)\n",
    "        j = int(threshold * inputs.numel())\n",
    "\n",
    "        # flat_out and mask access the same memory.\n",
    "        flat_out = mask.flatten()\n",
    "        flat_out[idx[j:]] = 0\n",
    "        flat_out[idx[:j]] = 1\n",
    "        ctx.save_for_backward(inputs, mask)\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, mask = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[mask == 0] = 0  # Only pass gradients for elements that are 1 in the mask\n",
    "        return grad_input, None\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config, mask_scale=1.0):\n",
    "        super().__init__()\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.out = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "        # Trainable mask scores for attention heads\n",
    "        self.mask_score_heads_1 = nn.Parameter(torch.Tensor(self.num_attention_heads))\n",
    "        self.mask_score_heads_2 = nn.Parameter(torch.Tensor(self.num_attention_heads))\n",
    "\n",
    "        self.mask_scale = mask_scale\n",
    "        self.init_mask()\n",
    "\n",
    "        self.topk_threshold = config.topk_threshold\n",
    "\n",
    "        # Gate for selecting mask: small two-layer fully connected network\n",
    "        self.gate_heads = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 2)  # 2 possible masks for heads\n",
    "        )\n",
    "\n",
    "    def init_mask(self):\n",
    "        init.constant_(self.mask_score_heads_1, val=self.mask_scale)\n",
    "        init.constant_(self.mask_score_heads_2, val=self.mask_scale)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Generate the corresponding binary mask for attention heads\n",
    "        mask_heads_1 = TopKBinarizer.apply(self.mask_score_heads_1, self.topk_threshold)\n",
    "        mask_heads_2 = TopKBinarizer.apply(self.mask_score_heads_2, self.topk_threshold)\n",
    "\n",
    "        # Compute the gate scores for attention heads\n",
    "        gate_scores_heads = self.gate_heads(hidden_states.mean(dim=1))\n",
    "        gate_probs_heads = F.softmax(gate_scores_heads, dim=-1)\n",
    "\n",
    "        # Select the mask with the highest probability for attention heads\n",
    "        selected_mask_index_heads = torch.argmax(gate_probs_heads, dim=1)\n",
    "\n",
    "        if selected_mask_index_heads[0] == 0:\n",
    "            selected_mask_heads = mask_heads_1\n",
    "        else:\n",
    "            selected_mask_heads = mask_heads_2\n",
    "\n",
    "        # Apply mask to the attention heads\n",
    "        # `selected_mask_heads` has shape [num_attention_heads]\n",
    "        # We need to expand it to [batch_size, num_attention_heads, seq_length, seq_length]\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "        masked_attention_heads = selected_mask_heads.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand(batch_size, self.num_attention_heads, seq_length, seq_length)\n",
    "\n",
    "        # Forward pass without modifying q, k, v\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Apply mask to the attention probs\n",
    "        attention_probs = attention_probs * masked_attention_heads\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config, mask_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, 4 * config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4 * config.hidden_size, config.hidden_size)\n",
    "\n",
    "        # Trainable mask scores\n",
    "        self.mask_score_1 = nn.Parameter(torch.Tensor(4 * config.hidden_size, config.hidden_size))\n",
    "        self.mask_score_2 = nn.Parameter(torch.Tensor(4 * config.hidden_size, config.hidden_size))\n",
    "\n",
    "        self.mask_scale = mask_scale\n",
    "        self.init_mask()\n",
    "\n",
    "        self.topk_threshold = config.topk_threshold\n",
    "\n",
    "        # Gate for selecting mask: small two-layer fully connected network\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def init_mask(self):\n",
    "        init.constant_(self.mask_score_1, val=self.mask_scale)\n",
    "        init.constant_(self.mask_score_2, val=self.mask_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate the corresponding binary mask\n",
    "        mask1 = TopKBinarizer.apply(self.mask_score_1, self.topk_threshold)\n",
    "        mask2 = TopKBinarizer.apply(self.mask_score_2, self.topk_threshold)\n",
    "\n",
    "        # Compute the gate scores\n",
    "        gate_scores = self.gate_mlp(x.mean(dim=1))  # Using mean of x as a simple representation\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "\n",
    "        # Select the mask with the highest probability\n",
    "        selected_mask_index = torch.argmax(gate_probs, dim=1)\n",
    "\n",
    "        if selected_mask_index[0] == 0:\n",
    "            selected_mask = mask1\n",
    "        else:\n",
    "            selected_mask = mask2\n",
    "\n",
    "        # Apply mask to the dense layer's weights\n",
    "        masked_weights = self.fc1.weight * selected_mask\n",
    "\n",
    "        # Forward pass with masked weights\n",
    "        x = F.linear(x, masked_weights, self.fc1.bias)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, mask_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = SelfAttention(config, mask_scale)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config, mask_scale)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        attn_output = self.attn(self.ln1(x), attention_mask)\n",
    "        x = x + attn_output\n",
    "        mlp_output = self.mlp(self.ln2(x))\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class BERTConfig:\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 4 * hidden_size\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    num_hidden_layers: int = 12\n",
    "    vocab_size: int = 30522\n",
    "    max_position_embeddings: int = 512\n",
    "    topk_threshold: float = 0.5\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, config, mask_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([Block(config, mask_scale) for _ in range(config.num_hidden_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        token_embeddings = self.embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        x = token_embeddings + position_embeddings\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, mask_scale=1.0):\n",
    "        \"\"\"Loads pretrained BERT model weights from HuggingFace\"\"\"\n",
    "        print(f\"Loading weights from pretrained BERT: {model_type}\")\n",
    "\n",
    "        # Instantiate the BERT model\n",
    "        config_args = {\n",
    "            'bert-base-uncased': dict(num_hidden_layers=12, num_attention_heads=12, hidden_size=768, intermediate_size=4 * 768),\n",
    "            'bert-large-uncased': dict(num_hidden_layers=24, num_attention_heads=16, hidden_size=1024, intermediate_size=4 * 1024),\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 30522\n",
    "        config_args['max_position_embeddings'] = 512\n",
    "        config = BERTConfig(**config_args)\n",
    "        model = BERT(config, mask_scale)\n",
    "\n",
    "        # Load HuggingFace BERT model for weight extraction\n",
    "        model_hf = BertModel.from_pretrained(model_type)\n",
    "\n",
    "        # Copy weights from HuggingFace model to our model\n",
    "        pretrained_dict = model_hf.state_dict()\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # Filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "\n",
    "        # Overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "        # Load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class QuestionAnsweringModel(nn.Module):\n",
    "    def __init__(self, model_type, mask_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.bert = BERT.from_pretrained(model_type, mask_scale)\n",
    "        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2)  # For start and end logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, start_positions=None, end_positions=None):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get logits for start and end positions\n",
    "        logits = self.qa_outputs(outputs)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        outputs = (start_logits, end_logits)\n",
    "        \n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # Compute loss\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,) + outputs\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained BERT: bert-base-uncased\n",
      "Answer: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_data(question, context, tokenizer, max_length=512):\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Initialize model\n",
    "model = QuestionAnsweringModel('bert-base-uncased')\n",
    "\n",
    "# Sample data\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"The capital of France is Paris.\"\n",
    "input_ids, attention_mask = prepare_data(question, context, tokenizer)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    start_logits, end_logits = outputs\n",
    "\n",
    "# Get the most likely beginning and end of answer span\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Convert token ids to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index+1])\n",
    "\n",
    "# Join tokens to form the answer\n",
    "answer = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained BERT: bert-base-uncased\n",
      "tensor([[[ 0.7452,  1.4914,  2.3781,  ..., -0.6299, -1.5615, -0.6429],\n",
      "         [ 1.8190, -0.7379, -1.7863,  ..., -2.0617,  0.6686,  0.7144],\n",
      "         [ 1.9354,  0.9880,  0.2105,  ...,  1.1575, -0.5406,  1.0018],\n",
      "         ...,\n",
      "         [ 0.9247, -0.0671, -1.1082,  ..., -0.2936,  0.2140, -0.4268],\n",
      "         [ 1.1636,  1.1287,  0.9556,  ..., -0.6875,  0.3120,  2.1783],\n",
      "         [ 0.7863,  0.4213,  0.9697,  ...,  0.0096,  0.5468, -0.6275]]])\n"
     ]
    }
   ],
   "source": [
    "# 加载BERT分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 准备输入数据\n",
    "text = \"Hello, how are you? I am using a simplified BERT model for inference.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 提取输入ID和注意力掩码\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# 加载预训练模型\n",
    "model = BERT.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 模型推理\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 输出结果\n",
    "print(outputs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
